# -*- coding: utf-8 -*-

import os
import random
import helpers
import numpy as np

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Variable

class Generator(nn.Module):
    """Generator """
    def __init__(self, num_emb, emb_dim, hidden_dim, use_cuda):
        super(Generator, self).__init__()
        self.num_emb = num_emb
        self.emb_dim = emb_dim
        self.hidden_dim = hidden_dim
        self.use_cuda = use_cuda
        self.emb = nn.Embedding(num_emb, emb_dim)
        self.lstm = nn.LSTM(emb_dim, hidden_dim, batch_first=True)
        self.lin = nn.Linear(hidden_dim, num_emb)
        self.softmax = nn.LogSoftmax()
        self.init_params()

    def forward(self, x):
        """
        Args:
            x: (batch_size, seq_len), sequence of tokens generated by generator
        """
        emb = self.emb(x)
        h0, c0 = self.init_hidden(x.size(0))
        output, (h, c) = self.lstm(emb, (h0, c0))
        pred = self.softmax(self.lin(output.contiguous().view(-1, self.hidden_dim)))
        return pred

    def step(self, x, h, c):
        """
        Args:
            x: (batch_size,  1), sequence of tokens generated by generator
            h: (1, batch_size, hidden_dim), lstm hidden state
            c: (1, batch_size, hidden_dim), lstm cell state
        """
        emb = self.emb(x)
        output, (h, c) = self.lstm(emb, (h, c))
        pred = F.softmax(self.lin(output.view(-1, self.hidden_dim)), dim=1)
        return pred, h, c


    def init_hidden(self, batch_size):
        h = Variable(torch.zeros((1, batch_size, self.hidden_dim)))
        c = Variable(torch.zeros((1, batch_size, self.hidden_dim)))
        if self.use_cuda:
            h, c = h.cuda(), c.cuda()
        return h, c

    def init_params(self):
        for param in self.parameters():
            param.data.uniform_(-0.05, 0.05)

    def sample(self, batch_size, seq_len, x=None):
        res = []
        flag = False  # whether sample from zero
        if x is None:
            flag = True
        if flag:
            x = Variable(torch.zeros((batch_size, 1)).long())
        if self.use_cuda:
            x = x.cuda()
        h, c = self.init_hidden(batch_size)
        samples = []
        if flag:
            for i in range(seq_len):
                output, h, c = self.step(x, h, c)
                x = output.multinomial(1)
                samples.append(x)
        else:
            given_len = x.size(1)
            lis = x.chunk(x.size(1), dim=1)
            for i in range(given_len):
                output, h, c = self.step(lis[i], h, c)
                samples.append(lis[i])
            x = output.multinomial(1)
            for i in range(given_len, seq_len):
                samples.append(x)
                output, h, c = self.step(x, h, c)
                x = output.multinomial(1)
        output = torch.cat(samples, dim=1)
        return output


    def sample_with_condition(self, batch_size, seq_len, x=None, strain_type=1, strain_rate=0.5, pinyin_rate=999):
        generated_location = 0
        if strain_type == 1:
            poem_strain = '平平仄仄仄平平仄平仄平仄仄仄平仄平仄仄平平仄'
        flag = False  # whether sample from zero
        if x is None:
            flag = True
        if flag:
            x = Variable(torch.zeros((batch_size, 1)).long())
        if self.use_cuda:
            x = x.cuda()
        h, c = self.init_hidden(batch_size)
        samples = []
        if flag:
            for i in range(seq_len):
                output, h, c = self.step(x, h, c)
                x = output.multinomial(1)
                samples.append(x)
        else:
            given_len = x.size(1)
            generated_location = given_len
            lis = x.chunk(x.size(1), dim=1)
            for i in range(given_len):
                output, h, c = self.step(lis[i], h, c)
                samples.append(lis[i])
            pinyin_list = []
            for i in range(output.size(0)):
                if generated_location >= 5:
                    pinyin_position = 5
                    x_target = x[i, pinyin_position-1].item()
                    x_target_pinyin = helpers.find_pinyin(x_target)
                    pinyin_list.append(x_target_pinyin)
                else:
                    pass
                right_index = helpers.find_right_word(strains=poem_strain[given_len+1], use_pinyin=False)
                output[i, right_index] += strain_rate
            x = output.multinomial(1)
            generated_location += 1
            cursor = 0
            for i in range(given_len, seq_len):
                samples.append(x)
                output, h, c = self.step(x, h, c)
                temp_cursor = 0
                for j in range(output.size(0)):
                    right_index = helpers.find_right_word(strains=poem_strain[given_len + i], use_pinyin=False)
                    output[j, right_index] += strain_rate
                    if cursor<=output.size(0)-1:
                        if generated_location >= 5:
                            # print("hi", generated_location, j, samples[-1][j])
                            pinyin_position = 5
                            x_target = samples[-1][j].item()
                            # print("hello", x_target)
                            x_target_pinyin = helpers.find_pinyin(x_target)
                            pinyin_list.append(x_target_pinyin)
                            cursor+=1
                        else:
                            pass
                    if generated_location in [9, 19]:
                        print("im here", generated_location, pinyin_list[j])
                        right_index = helpers.find_right_word(use_strains=False, pinyin=str(pinyin_list[temp_cursor]))
                        temp_cursor += 1
                        print(right_index)
                        output[j, right_index[0]] += pinyin_rate
                        print(output[j].sort(0,descending=True))
                        print("+"*78)
                        # print(output)
                x = output.multinomial(1)
                generated_location += 1
        output = torch.cat(samples, dim=1)
        return output


# if __name__ == '__main__':
